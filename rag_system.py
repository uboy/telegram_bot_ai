"""
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π
"""
import os
import json
import logging
from typing import List, Dict, Optional
from datetime import datetime
import numpy as np
from database import Base, Session, KnowledgeBase, KnowledgeChunk, KnowledgeImportLog

logger = logging.getLogger(__name__)

HAS_EMBEDDINGS = False
HAS_RERANKER = False
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    import faiss
    HAS_EMBEDDINGS = True
    # –ü–æ–¥–∞–≤–∏—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ hf_xet, –µ—Å–ª–∏ –ø–∞–∫–µ—Ç –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    import warnings
    warnings.filterwarnings('ignore', message='.*hf_xet.*', category=RuntimeWarning)
except ImportError:
    logger.warning("sentence-transformers –∏ faiss –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. RAG –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.")


# –ö–ª–∞—Å—Å—ã KnowledgeBase –∏ KnowledgeChunk –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –∏–∑ database.py


class RAGSystem:
    """–°–∏—Å—Ç–µ–º–∞ RAG –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, model_name: str = None):
        global HAS_EMBEDDINGS, HAS_RERANKER
        
        # –ü–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
        if model_name is None:
            try:
                from config import RAG_MODEL_NAME
                model_name = RAG_MODEL_NAME
            except ImportError:
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        
        self.model_name = model_name
        self.encoder = None
        self.index = None
        self.chunks = []
        self.session = Session()
        self.reranker = None
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ–¥ rerank (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–ø–≥—Ä–µ–π–¥)
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 100 –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π
        try:
            from config import RAG_MAX_CANDIDATES
            self.max_candidates = RAG_MAX_CANDIDATES
        except ImportError:
            self.max_candidates = int(os.getenv("RAG_MAX_CANDIDATES", "100"))
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å
        try:
            from config import RAG_ENABLE
            if RAG_ENABLE is False:
                HAS_EMBEDDINGS = False
                logger.info("‚ÑπÔ∏è RAG –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
                return
        except ImportError:
            pass  # RAG_ENABLE –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        
        if HAS_EMBEDDINGS:
            try:
                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É –º–æ–¥–µ–ª–µ–π (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º HF_HOME –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏–Ω–∞—á–µ BOT_DATA_DIR
                cache_dir = os.getenv("HF_HOME") or os.path.join(os.getenv("BOT_DATA_DIR", "/app/data"), "cache", "huggingface")
                os.makedirs(cache_dir, exist_ok=True)
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ
                import glob
                # sentence-transformers –∫–µ—à–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –≤ cache_dir/models--model_name
                model_cache_name = model_name.replace("/", "--")
                model_cache_path = os.path.join(cache_dir, f"models--{model_cache_name}")
                
                if os.path.exists(model_cache_path):
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞: {model_name}")
                    logger.info(f"   –ö—ç—à: {model_cache_path}")
                else:
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
                    logger.info("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")
                    logger.info(f"   –ö—ç—à –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {cache_dir}")
                
                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π (CPU –∏–ª–∏ GPU)
                try:
                    from config import RAG_DEVICE
                    device = RAG_DEVICE
                except ImportError:
                    device = os.getenv("RAG_DEVICE", "cpu")
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ GPU
                if device.startswith("cuda"):
                    try:
                        import torch
                        
                        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ CUDA
                        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA...")
                        logger.info(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
                        logger.info(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch: {torch.cuda.is_available()}")
                        
                        if torch.cuda.is_available():
                            logger.info(f"   CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
                            logger.info(f"   cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else '–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}")
                            logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
                            for i in range(torch.cuda.device_count()):
                                logger.info(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                            logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {device} (–¥–æ—Å—Ç—É–ø–Ω–æ {torch.cuda.device_count()} —É—Å—Ç—Ä–æ–π—Å—Ç–≤)")
                        else:
                            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                            logger.warning(f"‚ö†Ô∏è CUDA –∑–∞–ø—Ä–æ—à–µ–Ω–∞ ({device}), –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch.")
                            logger.warning(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__} (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA, –Ω–æ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
                            
                            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                            nvidia_devices_found = False
                            try:
                                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ NVIDIA –≤ /dev
                                nvidia_devices = [f for f in os.listdir('/dev') if f.startswith('nvidia')]
                                if nvidia_devices:
                                    nvidia_devices_found = True
                                    logger.warning(f"   ‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ NVIDIA –Ω–∞–π–¥–µ–Ω—ã –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ: {', '.join(nvidia_devices)}")
                                else:
                                    logger.warning(f"   ‚ùå –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ NVIDIA –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ /dev (–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ GPU)")
                            except Exception as e:
                                logger.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å /dev: {e}")
                            
                            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å nvidia-smi
                            nvidia_smi_available = False
                            try:
                                import subprocess
                                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
                                if result.returncode == 0:
                                    nvidia_smi_available = True
                                    logger.warning(f"   ‚úÖ nvidia-smi —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ")
                                    logger.warning(f"   ‚ö†Ô∏è GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ, –Ω–æ PyTorch –Ω–µ –≤–∏–¥–∏—Ç CUDA.")
                                    logger.warning(f"   üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                                    logger.warning(f"      1. –í–µ—Ä—Å–∏—è CUDA –≤ PyTorch ({torch.version.cuda if hasattr(torch.version, 'cuda') else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞'}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –≤–µ—Ä—Å–∏–µ–π –¥—Ä–∞–π–≤–µ—Ä–∞")
                                    logger.warning(f"      2. –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π CUDA")
                                else:
                                    logger.warning(f"   ‚ùå nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                            except FileNotFoundError:
                                logger.warning(f"   ‚ùå nvidia-smi –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ (GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ Docker)")
                            except (subprocess.TimeoutExpired, Exception) as e:
                                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ nvidia-smi: {e}")
                            
                            # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                            if not nvidia_devices_found and not nvidia_smi_available:
                                logger.warning(f"   üîß –†–ï–®–ï–ù–ò–ï: –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–æ—Å—Ç—É–ø –∫ GPU –≤ Docker:")
                                logger.warning(f"      1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ nvidia-container-toolkit –Ω–∞ —Ö–æ—Å—Ç–µ")
                                logger.warning(f"      2. –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–µ–∫—Ü–∏—é 'deploy' –≤ docker-compose.yml (—Å–µ—Ä–≤–∏—Å 'bot')")
                                logger.warning(f"      3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Docker: sudo systemctl restart docker")
                                logger.warning(f"      4. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: docker-compose up -d --build")
                            elif nvidia_devices_found or nvidia_smi_available:
                                logger.warning(f"   üîß –†–ï–®–ï–ù–ò–ï: GPU –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ, –Ω–æ PyTorch –µ–≥–æ –Ω–µ –≤–∏–¥–∏—Ç.")
                                logger.warning(f"      –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π CUDA –≤ PyTorch –∏ –¥—Ä–∞–π–≤–µ—Ä–∞.")
                            
                            logger.warning(f"   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                            device = "cpu"
                    except ImportError:
                        logger.warning("‚ö†Ô∏è PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å CUDA. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                        device = "cpu"
                
                # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—É—Ç–∏ –∫ –∫—ç—à—É –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
                # SentenceTransformer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–µ—à –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
                self.encoder = SentenceTransformer(model_name, cache_folder=cache_dir, device=device)
                self.dimension = self.encoder.get_sentence_embedding_dimension()
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.dimension}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")

                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–ø–≥—Ä–µ–π–¥ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞)
                try:
                    from config import RAG_RERANK_MODEL
                    rerank_model_name = RAG_RERANK_MODEL
                except ImportError:
                    rerank_model_name = os.getenv(
                        "RAG_RERANK_MODEL",
                        "cross-encoder/ms-marco-MiniLM-L-6-v2",
                    )
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–µ—à –¥–ª—è reranker
                rerank_cache_name = rerank_model_name.replace("/", "--")
                rerank_cache_path = os.path.join(cache_dir, f"models--{rerank_cache_name}")
                
                if os.path.exists(rerank_cache_path):
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ reranker –∏–∑ –∫—ç—à–∞: {rerank_model_name}")
                else:
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ reranker: {rerank_model_name}...")
                
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —á—Ç–æ –∏ –¥–ª—è encoder
                    self.reranker = CrossEncoder(rerank_model_name, cache_folder=cache_dir, device=device)
                    HAS_RERANKER = True
                    logger.info(f"‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ: {rerank_model_name} (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
                except Exception as rerank_error:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker ({rerank_model_name}): {rerank_error}")
                    logger.info("   –ü–æ–∏—Å–∫ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ reranker'–∞ (—Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)")
                    self.reranker = None
                    HAS_RERANKER = False

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                logger.info("   –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
                self.encoder = None
                HAS_EMBEDDINGS = False
    
    def reload_models(self) -> Dict[str, bool]:
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Ä–∞–Ω–∫–∏–Ω–≥–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –≤ —Ä–∞–Ω—Ç–∞–π–º–µ.
        
        Returns:
            dict —Å –∫–ª—é—á–∞–º–∏ 'embedding' –∏ 'reranker' –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ True/False (—É—Å–ø–µ—Ö/–æ—à–∏–±–∫–∞)
        """
        global HAS_EMBEDDINGS, HAS_RERANKER
        result = {'embedding': False, 'reranker': False}
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (–Ω–µ —Ç–æ–ª—å–∫–æ —Ñ–ª–∞–≥ HAS_EMBEDDINGS)
        try:
            from sentence_transformers import SentenceTransformer, CrossEncoder
            libraries_available = True
        except ImportError:
            libraries_available = False
        
        if not libraries_available:
            logger.warning("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
            return result
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω –ª–∏ RAG –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        try:
            from config import RAG_ENABLE
            if RAG_ENABLE is False:
                logger.warning("‚ö†Ô∏è RAG –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (RAG_ENABLE=false), –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
                return result
        except ImportError:
            pass  # RAG_ENABLE –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        
        try:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏
            if self.encoder:
                del self.encoder
                self.encoder = None
            if self.reranker:
                del self.reranker
                self.reranker = None
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU
            import gc
            gc.collect()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            try:
                from config import RAG_MODEL_NAME, RAG_RERANK_MODEL, RAG_DEVICE
                new_model_name = RAG_MODEL_NAME
                new_rerank_model = RAG_RERANK_MODEL
                device = RAG_DEVICE
            except ImportError:
                new_model_name = os.getenv("RAG_MODEL_NAME", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                new_rerank_model = os.getenv("RAG_RERANK_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")
                device = os.getenv("RAG_DEVICE", "cpu")
            
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É
            cache_dir = os.getenv("HF_HOME") or os.path.join(os.getenv("BOT_DATA_DIR", "/app/data"), "cache", "huggingface")
            os.makedirs(cache_dir, exist_ok=True)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ GPU
            if device.startswith("cuda"):
                try:
                    import torch
                    
                    if not torch.cuda.is_available():
                        logger.warning(f"‚ö†Ô∏è CUDA –∑–∞–ø—Ä–æ—à–µ–Ω–∞ ({device}), –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å nvidia-smi –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                        try:
                            import subprocess
                            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
                            if result.returncode == 0:
                                logger.warning(f"   ‚ö†Ô∏è GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ, –Ω–æ PyTorch –Ω–µ –≤–∏–¥–∏—Ç CUDA.")
                                logger.warning(f"   üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ –≤–µ—Ä—Å–∏—è PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
                        except:
                            pass
                        device = "cpu"
                    else:
                        logger.info(f"üöÄ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Å GPU: {device} (–¥–æ—Å—Ç—É–ø–Ω–æ {torch.cuda.device_count()} —É—Å—Ç—Ä–æ–π—Å—Ç–≤)")
                except ImportError:
                    logger.warning("‚ö†Ô∏è PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å CUDA. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                    device = "cpu"
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            try:
                logger.info(f"üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {new_model_name}")
                self.encoder = SentenceTransformer(new_model_name, cache_folder=cache_dir, device=device)
                self.dimension = self.encoder.get_sentence_embedding_dimension()
                self.model_name = new_model_name
                result['embedding'] = True
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.dimension}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}", exc_info=True)
                result['embedding'] = False
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–π reranker
            try:
                logger.info(f"üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ reranker: {new_rerank_model}")
                self.reranker = CrossEncoder(new_rerank_model, cache_folder=cache_dir, device=device)
                HAS_RERANKER = True
                result['reranker'] = True
                logger.info(f"‚úÖ Reranker –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
            except Exception as rerank_error:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å reranker ({new_rerank_model}): {rerank_error}")
                self.reranker = None
                HAS_RERANKER = False
                result['reranker'] = False
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –ø–æ–∏—Å–∫–µ (–æ–Ω –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            self.index = None
            self.chunks = []
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π RAG: {e}", exc_info=True)
        
        return result
    
    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞"""
        if not HAS_EMBEDDINGS or not self.encoder:
            return None
        try:
            return self.encoder.encode(text, convert_to_numpy=True)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None
    
    def _load_index(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        if not HAS_EMBEDDINGS:
            return
        
        chunks = self.session.query(KnowledgeChunk).all()
        if not chunks:
            return
        
        self.chunks = chunks
        embeddings = []
        valid_chunks = []
        
        for chunk in chunks:
            if chunk.embedding:
                try:
                    embedding = np.array(json.loads(chunk.embedding))
                    embeddings.append(embedding)
                    valid_chunks.append(chunk)
                except:
                    continue
        
        if embeddings:
            self.chunks = valid_chunks
            embeddings = np.array(embeddings).astype('float32')
            self.dimension = embeddings.shape[1]
            self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(embeddings)
    
    def add_knowledge_base(self, name: str, description: str = "") -> KnowledgeBase:
        """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        kb = KnowledgeBase(name=name, description=description)
        self.session.add(kb)
        self.session.commit()
        return kb
    
    def get_knowledge_base(self, name_or_id) -> Optional[KnowledgeBase]:
        """–ü–æ–ª—É—á–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ –∏–º–µ–Ω–∏ –∏–ª–∏ ID"""
        if isinstance(name_or_id, int):
            return self.session.query(KnowledgeBase).filter_by(id=name_or_id).first()
        return self.session.query(KnowledgeBase).filter_by(name=name_or_id).first()
    
    def list_knowledge_bases(self) -> List[KnowledgeBase]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π"""
        return self.session.query(KnowledgeBase).all()
    
    def add_chunk(self, knowledge_base_id: int, content: str, 
                  source_type: str = "text", source_path: str = "",
                  metadata: Optional[Dict] = None) -> KnowledgeChunk:
        """–î–æ–±–∞–≤–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∑–Ω–∞–Ω–∏—è"""
        embedding = self._get_embedding(content)
        embedding_json = json.dumps(embedding.tolist()) if embedding is not None else None
        
        chunk = KnowledgeChunk(
            knowledge_base_id=knowledge_base_id,
            content=content,
            chunk_metadata=json.dumps(metadata or {}),
            embedding=embedding_json,
            source_type=source_type,
            source_path=source_path
        )
        self.session.add(chunk)
        self.session.commit()
        
        # –û–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å
        if embedding is not None and HAS_EMBEDDINGS:
            if self.index is None:
                self.dimension = embedding.shape[0]
                self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(embedding.reshape(1, -1).astype('float32'))
            self.chunks.append(chunk)
        
        return chunk
    
    def search(self, query: str, knowledge_base_id: Optional[int] = None, 
               top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        if not HAS_EMBEDDINGS or not self.encoder:
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            return self._simple_search(query, knowledge_base_id, top_k)
        
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        query_embedding = self._get_embedding(query)
        if query_embedding is None or self.index is None:
            return self._simple_search(query, knowledge_base_id, top_k)
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω
        if not self.chunks:
            self._load_index()
        
        if self.index is None or len(self.chunks) == 0:
            return []
        
        # –ü–æ–∏—Å–∫: —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–±–∏—Ä–∞–µ–º –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –ø—É–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∑–∞—Ç–µ–º –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ reranker
        # –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤—ã–≤–∞–µ–º –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º top_k (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–ø–≥—Ä–µ–π–¥: top-50 -> rerank -> top-k).
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        candidate_k = min(self.max_candidates, len(self.chunks))
        distances, indices = self.index.search(query_embedding, candidate_k)
        
        candidates = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                if knowledge_base_id is not None and chunk.knowledge_base_id != knowledge_base_id:
                    continue
                candidates.append(
                    (
                        chunk,
                        float(distances[0][i]),
                    )
                )

        if not candidates:
            return []

        # –ï—Å–ª–∏ –µ—Å—Ç—å reranker, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –±–µ—Ä–µ–º top_k –ø–æ score
        if HAS_RERANKER and self.reranker is not None:
            try:
                pairs = [[query, c.content] for (c, _) in candidates]
                scores = self.reranker.predict(pairs)
                # –°–æ–µ–¥–∏–Ω—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –∏—Ö rerank-score
                scored = list(zip(candidates, scores))
                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score –ø–æ —É–±—ã–≤–∞–Ω–∏—é
                scored.sort(key=lambda x: x[1], reverse=True)
                top = scored[: top_k]

                logger.debug("Reranker –ø—Ä–∏–º–µ–Ω–µ–Ω: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ %d –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –≤—ã–±—Ä–∞–Ω–æ top-%d", len(candidates), len(top))
                if top:
                    logger.debug("–õ—É—á—à–∏–π rerank_score: %.4f, —Ö—É–¥—à–∏–π: %.4f", top[0][1], top[-1][1])

                results = []
                for ((chunk, distance), score) in top:
                    results.append(
                        {
                            "content": chunk.content,
                            "metadata": json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {},
                            "source_type": chunk.source_type,
                            "source_path": chunk.source_path,
                            # –û—Å—Ç–∞–≤–ª—è–µ–º –æ–±–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ–π –æ—Ç–ª–∞–¥–∫–∏
                            "distance": float(distance),
                            "rerank_score": float(score),
                        }
                    )
                return results
            except Exception as e:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã reranker, –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–µ–≥–æ: %s", e)
                import traceback
                logger.debug("Traceback reranker: %s", traceback.format_exc())
                # fallthrough –∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é

        # –ï—Å–ª–∏ reranker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (top_k –ø–æ distance)
        results = []
        for i, (chunk, distance) in enumerate(candidates[: top_k]):
            results.append(
                {
                    "content": chunk.content,
                    "metadata": json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {},
                    "source_type": chunk.source_type,
                    "source_path": chunk.source_path,
                    "distance": float(distance),
                }
            )
        return results
    
    def _simple_search(self, query: str, knowledge_base_id: Optional[int] = None, 
                      top_k: int = 5) -> List[Dict]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        query_lower = query.lower()
        chunks = self.session.query(KnowledgeChunk).all()
        
        scored_chunks = []
        for chunk in chunks:
            if knowledge_base_id and chunk.knowledge_base_id != knowledge_base_id:
                continue
            
            content_lower = chunk.content.lower()
            score = sum(1 for word in query_lower.split() if word in content_lower)
            if score > 0:
                scored_chunks.append((score, chunk))
        
        scored_chunks.sort(reverse=True, key=lambda x: x[0])
        
        results = []
        for score, chunk in scored_chunks[:top_k]:
            results.append({
                'content': chunk.content,
                'metadata': json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {},
                'source_type': chunk.source_type,
                'source_path': chunk.source_path,
                'distance': 1.0 / (score + 1)  # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            })
        
        return results
    
    def delete_knowledge_base(self, knowledge_base_id: int) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π, –≤—Å–µ –µ—ë —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ –∂—É—Ä–Ω–∞–ª –∑–∞–≥—Ä—É–∑–æ–∫"""
        # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        chunks = self.session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
        for chunk in chunks:
            self.session.delete(chunk)
        
        # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∂—É—Ä–Ω–∞–ª–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –¥–ª—è —ç—Ç–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        logs = self.session.query(KnowledgeImportLog).filter_by(knowledge_base_id=knowledge_base_id).all()
        for log in logs:
            self.session.delete(log)
        
        # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—É –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        kb = self.session.query(KnowledgeBase).filter_by(id=knowledge_base_id).first()
        if kb:
            self.session.delete(kb)
            self.session.commit()
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
            self.chunks = []
            self.index = None
            self._load_index()
            return True
        return False
    
    def clear_knowledge_base(self, knowledge_base_id: int) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –æ—Ç –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ –∂—É—Ä–Ω–∞–ª–∞ –∑–∞–≥—Ä—É–∑–æ–∫"""
        # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        chunks = self.session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
        for chunk in chunks:
            self.session.delete(chunk)
        
        # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∂—É—Ä–Ω–∞–ª–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –¥–ª—è —ç—Ç–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        logs = self.session.query(KnowledgeImportLog).filter_by(knowledge_base_id=knowledge_base_id).all()
        for log in logs:
            self.session.delete(log)
        
        self.session.commit()
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        self.chunks = []
        self.index = None
        self._load_index()
        return True

    def delete_chunks_by_source_exact(
        self,
        knowledge_base_id: int,
        source_type: str,
        source_path: str,
    ) -> int:
        """
        –£–¥–∞–ª–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ —Ä–∞–º–∫–∞—Ö –ë–ó.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –∑–∞–º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ.
        """
        if not source_path:
            return 0

        q = (
            self.session.query(KnowledgeChunk)
            .filter_by(
                knowledge_base_id=knowledge_base_id,
                source_type=source_type,
                source_path=source_path,
            )
        )
        chunks = q.all()
        deleted = 0
        for chunk in chunks:
            self.session.delete(chunk)
            deleted += 1

        if deleted:
            self.session.commit()
            # –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª —Ç–µ–∫—É—â–µ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –ë–î
            self.chunks = []
            self.index = None
            self._load_index()

        return deleted

    def delete_chunks_by_source_prefix(
        self,
        knowledge_base_id: int,
        source_type: str,
        source_prefix: str,
    ) -> int:
        """
        –£–¥–∞–ª–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–¥–Ω–æ–π –≤–∏–∫–∏).
        
        –≠—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è wiki-—Å–∫—Ä–µ–ø–µ—Ä–æ–º –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –≤–∏–∫–∏ –±–µ–∑ –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
        """
        if not source_prefix:
            return 0

        # –ù–∞–π—Ç–∏ –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏ —Å –Ω—É–∂–Ω—ã–º —Ç–∏–ø–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        query = (
            self.session.query(KnowledgeChunk)
            .filter_by(knowledge_base_id=knowledge_base_id, source_type=source_type)
        )
        chunks = query.all()
        deleted = 0

        for chunk in chunks:
            if chunk.source_path and chunk.source_path.startswith(source_prefix):
                self.session.delete(chunk)
                deleted += 1

        if deleted:
            self.session.commit()
            # –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª —Ç–µ–∫—É—â–µ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –ë–î
            self.chunks = []
            self.index = None
            self._load_index()

        return deleted


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã
rag_system = RAGSystem()

