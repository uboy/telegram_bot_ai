"""
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –∑–Ω–∞–Ω–∏–π
"""
import os
import json
import logging
import threading
from typing import List, Dict, Optional
from datetime import datetime
from collections import defaultdict
import numpy as np
from shared.database import Base, Session, KnowledgeBase, KnowledgeChunk, KnowledgeImportLog, engine, get_session
from sqlalchemy import text, or_

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π lock –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞–ø–∏—Å–∏ –≤ –ë–î (SQLite –Ω–µ –ª—é–±–∏—Ç –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ writers)
_db_write_lock = threading.Lock()

HAS_EMBEDDINGS = False
HAS_RERANKER = False
try:
    from sentence_transformers import SentenceTransformer, CrossEncoder
    import faiss
    HAS_EMBEDDINGS = True
    # –ü–æ–¥–∞–≤–∏—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ hf_xet, –µ—Å–ª–∏ –ø–∞–∫–µ—Ç –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    import warnings
    warnings.filterwarnings('ignore', message='.*hf_xet.*', category=RuntimeWarning)
except ImportError:
    logger.warning("sentence-transformers –∏ faiss –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. RAG –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.")


# –ö–ª–∞—Å—Å—ã KnowledgeBase –∏ KnowledgeChunk –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –∏–∑ database.py


class RAGSystem:
    """–°–∏—Å—Ç–µ–º–∞ RAG –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    
    def __init__(self, model_name: str = None):
        global HAS_EMBEDDINGS, HAS_RERANKER
        
        # –ü–æ–ª—É—á–∏—Ç—å –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
        if model_name is None:
            try:
                from shared.config import RAG_MODEL_NAME
                model_name = RAG_MODEL_NAME
            except ImportError:
                model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        
        self.model_name = model_name
        self.encoder = None
        self.index = None  # –£—Å—Ç–∞—Ä–µ–≤—à–∏–π: –æ–¥–∏–Ω –∏–Ω–¥–µ–∫—Å –¥–ª—è –≤—Å–µ—Ö KB
        self.chunks = []  # –£—Å—Ç–∞—Ä–µ–≤—à–∏–π: –≤—Å–µ —á–∞–Ω–∫–∏ –≤–º–µ—Å—Ç–µ
        # –ò–Ω–¥–µ–∫—Å—ã –ø–æ –±–∞–∑–∞–º –∑–Ω–∞–Ω–∏–π (–¥–ª—è —Ä–∞–∑–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
        self.index_by_kb: Dict[int, faiss.Index] = {}
        self.chunks_by_kb: Dict[int, List[KnowledgeChunk]] = {}
        # –°–µ—Å—Å–∏–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –Ω–∞ –∫–∞–∂–¥—É—é –æ–ø–µ—Ä–∞—Ü–∏—é, –Ω–µ —Ö—Ä–∞–Ω–∏–º –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å–µ—Å—Å–∏—é
        self.reranker = None
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ–¥ rerank (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–ø–≥—Ä–µ–π–¥)
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 100 –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π
        try:
            from shared.config import RAG_MAX_CANDIDATES
            self.max_candidates = RAG_MAX_CANDIDATES
        except ImportError:
            self.max_candidates = int(os.getenv("RAG_MAX_CANDIDATES", "100"))
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å
        try:
            from shared.config import RAG_ENABLE
            if RAG_ENABLE is False:
                HAS_EMBEDDINGS = False
                logger.info("‚ÑπÔ∏è RAG –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
                return
        except ImportError:
            pass  # RAG_ENABLE –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        
        if HAS_EMBEDDINGS:
            try:
                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É –º–æ–¥–µ–ª–µ–π (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º HF_HOME –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏–Ω–∞—á–µ BOT_DATA_DIR
                cache_dir = os.getenv("HF_HOME") or os.path.join(os.getenv("BOT_DATA_DIR", "/app/data"), "cache", "huggingface")
                os.makedirs(cache_dir, exist_ok=True)
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ
                import glob
                # sentence-transformers –∫–µ—à–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –≤ cache_dir/models--model_name
                model_cache_name = model_name.replace("/", "--")
                model_cache_path = os.path.join(cache_dir, f"models--{model_cache_name}")
                
                if os.path.exists(model_cache_path):
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞: {model_name}")
                    logger.info(f"   –ö—ç—à: {model_cache_path}")
                else:
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
                    logger.info("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")
                    logger.info(f"   –ö—ç—à –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {cache_dir}")
                
                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π (CPU –∏–ª–∏ GPU)
                try:
                    from shared.config import RAG_DEVICE
                    device = RAG_DEVICE
                except ImportError:
                    device = os.getenv("RAG_DEVICE", "cpu")
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ GPU
                if device.startswith("cuda"):
                    try:
                        import torch
                        
                        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ CUDA
                        logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA...")
                        logger.info(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
                        logger.info(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch: {torch.cuda.is_available()}")
                        
                        if torch.cuda.is_available():
                            logger.info(f"   CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
                            logger.info(f"   cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else '–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}")
                            logger.info(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
                            for i in range(torch.cuda.device_count()):
                                logger.info(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                            logger.info(f"üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {device} (–¥–æ—Å—Ç—É–ø–Ω–æ {torch.cuda.device_count()} —É—Å—Ç—Ä–æ–π—Å—Ç–≤)")
                        else:
                            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                            logger.warning(f"‚ö†Ô∏è CUDA –∑–∞–ø—Ä–æ—à–µ–Ω–∞ ({device}), –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –≤ PyTorch.")
                            logger.warning(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__} (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA, –Ω–æ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
                            
                            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                            nvidia_devices_found = False
                            try:
                                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤ NVIDIA –≤ /dev
                                nvidia_devices = [f for f in os.listdir('/dev') if f.startswith('nvidia')]
                                if nvidia_devices:
                                    nvidia_devices_found = True
                                    logger.warning(f"   ‚úÖ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ NVIDIA –Ω–∞–π–¥–µ–Ω—ã –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ: {', '.join(nvidia_devices)}")
                                else:
                                    logger.warning(f"   ‚ùå –£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ NVIDIA –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ /dev (–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ GPU)")
                            except Exception as e:
                                logger.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å /dev: {e}")
                            
                            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å nvidia-smi
                            nvidia_smi_available = False
                            try:
                                import subprocess
                                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
                                if result.returncode == 0:
                                    nvidia_smi_available = True
                                    logger.warning(f"   ‚úÖ nvidia-smi —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ")
                                    logger.warning(f"   ‚ö†Ô∏è GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ, –Ω–æ PyTorch –Ω–µ –≤–∏–¥–∏—Ç CUDA.")
                                    logger.warning(f"   üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
                                    logger.warning(f"      1. –í–µ—Ä—Å–∏—è CUDA –≤ PyTorch ({torch.version.cuda if hasattr(torch.version, 'cuda') else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞'}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –≤–µ—Ä—Å–∏–µ–π –¥—Ä–∞–π–≤–µ—Ä–∞")
                                    logger.warning(f"      2. –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π CUDA")
                                else:
                                    logger.warning(f"   ‚ùå nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                            except FileNotFoundError:
                                logger.warning(f"   ‚ùå nvidia-smi –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ (GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ Docker)")
                            except (subprocess.TimeoutExpired, Exception) as e:
                                logger.warning(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ nvidia-smi: {e}")
                            
                            # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                            if not nvidia_devices_found and not nvidia_smi_available:
                                logger.warning(f"   üîß –†–ï–®–ï–ù–ò–ï: –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–æ—Å—Ç—É–ø –∫ GPU –≤ Docker:")
                                logger.warning(f"      1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ nvidia-container-toolkit –Ω–∞ —Ö–æ—Å—Ç–µ")
                                logger.warning(f"      2. –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å–µ–∫—Ü–∏—é 'deploy' –≤ docker-compose.yml (—Å–µ—Ä–≤–∏—Å 'bot')")
                                logger.warning(f"      3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Docker: sudo systemctl restart docker")
                                logger.warning(f"      4. –ü–µ—Ä–µ—Å–æ–±–µ—Ä–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: docker-compose up -d --build")
                            elif nvidia_devices_found or nvidia_smi_available:
                                logger.warning(f"   üîß –†–ï–®–ï–ù–ò–ï: GPU –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ, –Ω–æ PyTorch –µ–≥–æ –Ω–µ –≤–∏–¥–∏—Ç.")
                                logger.warning(f"      –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π CUDA –≤ PyTorch –∏ –¥—Ä–∞–π–≤–µ—Ä–∞.")
                            
                            logger.warning(f"   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                            device = "cpu"
                    except ImportError:
                        logger.warning("‚ö†Ô∏è PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å CUDA. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                        device = "cpu"
                
                # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—É—Ç–∏ –∫ –∫—ç—à—É –∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
                # SentenceTransformer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–µ—à –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
                self.encoder = SentenceTransformer(model_name, cache_folder=cache_dir, device=device)
                self.dimension = self.encoder.get_sentence_embedding_dimension()
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.dimension}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")

                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–ø–≥—Ä–µ–π–¥ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞)
                try:
                    from shared.config import RAG_RERANK_MODEL
                    rerank_model_name = RAG_RERANK_MODEL
                except ImportError:
                    rerank_model_name = os.getenv(
                        "RAG_RERANK_MODEL",
                        "cross-encoder/ms-marco-MiniLM-L-6-v2",
                    )
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–µ—à –¥–ª—è reranker
                rerank_cache_name = rerank_model_name.replace("/", "--")
                rerank_cache_path = os.path.join(cache_dir, f"models--{rerank_cache_name}")
                
                if os.path.exists(rerank_cache_path):
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ reranker –∏–∑ –∫—ç—à–∞: {rerank_model_name}")
                else:
                    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ reranker: {rerank_model_name}...")
                
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —á—Ç–æ –∏ –¥–ª—è encoder
                    self.reranker = CrossEncoder(rerank_model_name, cache_folder=cache_dir, device=device)
                    HAS_RERANKER = True
                    logger.info(f"‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ: {rerank_model_name} (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
                except Exception as rerank_error:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å reranker ({rerank_model_name}): {rerank_error}")
                    logger.info("   –ü–æ–∏—Å–∫ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ reranker'–∞ (—Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)")
                    self.reranker = None
                    HAS_RERANKER = False

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                logger.info("   –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
                self.encoder = None
                HAS_EMBEDDINGS = False
    
    def reload_models(self) -> Dict[str, bool]:
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Ä–∞–Ω–∫–∏–Ω–≥–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –≤ —Ä–∞–Ω—Ç–∞–π–º–µ.
        
        Returns:
            dict —Å –∫–ª—é—á–∞–º–∏ 'embedding' –∏ 'reranker' –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ True/False (—É—Å–ø–µ—Ö/–æ—à–∏–±–∫–∞)
        """
        global HAS_EMBEDDINGS, HAS_RERANKER
        result = {'embedding': False, 'reranker': False}
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (–Ω–µ —Ç–æ–ª—å–∫–æ —Ñ–ª–∞–≥ HAS_EMBEDDINGS)
        try:
            from sentence_transformers import SentenceTransformer, CrossEncoder
            libraries_available = True
        except ImportError:
            libraries_available = False
        
        if not libraries_available:
            logger.warning("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã, –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
            return result
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω –ª–∏ RAG –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        try:
            from shared.config import RAG_ENABLE
            if RAG_ENABLE is False:
                logger.warning("‚ö†Ô∏è RAG –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (RAG_ENABLE=false), –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
                return result
        except ImportError:
            pass  # RAG_ENABLE –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        
        try:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏
            if self.encoder:
                del self.encoder
                self.encoder = None
            if self.reranker:
                del self.reranker
                self.reranker = None
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU
            import gc
            gc.collect()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            try:
                from shared.config import RAG_MODEL_NAME, RAG_RERANK_MODEL, RAG_DEVICE
                new_model_name = RAG_MODEL_NAME
                new_rerank_model = RAG_RERANK_MODEL
                device = RAG_DEVICE
            except ImportError:
                new_model_name = os.getenv("RAG_MODEL_NAME", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                new_rerank_model = os.getenv("RAG_RERANK_MODEL", "cross-encoder/ms-marco-MiniLM-L-6-v2")
                device = os.getenv("RAG_DEVICE", "cpu")
            
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ –∫—ç—à—É
            cache_dir = os.getenv("HF_HOME") or os.path.join(os.getenv("BOT_DATA_DIR", "/app/data"), "cache", "huggingface")
            os.makedirs(cache_dir, exist_ok=True)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ GPU
            if device.startswith("cuda"):
                try:
                    import torch
                    
                    if not torch.cuda.is_available():
                        logger.warning(f"‚ö†Ô∏è CUDA –∑–∞–ø—Ä–æ—à–µ–Ω–∞ ({device}), –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å nvidia-smi –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                        try:
                            import subprocess
                            result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
                            if result.returncode == 0:
                                logger.warning(f"   ‚ö†Ô∏è GPU –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ, –Ω–æ PyTorch –Ω–µ –≤–∏–¥–∏—Ç CUDA.")
                                logger.warning(f"   üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ –≤–µ—Ä—Å–∏—è PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
                        except:
                            pass
                        device = "cpu"
                    else:
                        logger.info(f"üöÄ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Å GPU: {device} (–¥–æ—Å—Ç—É–ø–Ω–æ {torch.cuda.device_count()} —É—Å—Ç—Ä–æ–π—Å—Ç–≤)")
                except ImportError:
                    logger.warning("‚ö†Ô∏è PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å CUDA. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                    device = "cpu"
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            try:
                logger.info(f"üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {new_model_name}")
                self.encoder = SentenceTransformer(new_model_name, cache_folder=cache_dir, device=device)
                self.dimension = self.encoder.get_sentence_embedding_dimension()
                self.model_name = new_model_name
                result['embedding'] = True
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.dimension}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}", exc_info=True)
                result['embedding'] = False
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–π reranker
            try:
                logger.info(f"üîÑ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ reranker: {new_rerank_model}")
                self.reranker = CrossEncoder(new_rerank_model, cache_folder=cache_dir, device=device)
                HAS_RERANKER = True
                result['reranker'] = True
                logger.info(f"‚úÖ Reranker –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device})")
            except Exception as rerank_error:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å reranker ({new_rerank_model}): {rerank_error}")
                self.reranker = None
                HAS_RERANKER = False
                result['reranker'] = False
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –ø–æ–∏—Å–∫–µ (–æ–Ω –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            self.index = None
            self.chunks = []
            self.index_by_kb.clear()
            self.chunks_by_kb.clear()
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π RAG: {e}", exc_info=True)
        
        return result
    
    def _get_embedding(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞"""
        if not HAS_EMBEDDINGS or not self.encoder:
            return None
        try:
            return self.encoder.encode(text, convert_to_numpy=True)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None
    
    def _load_index(self, knowledge_base_id: Optional[int] = None):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–ø–æ KB –∏–ª–∏ –≤—Å–µ)"""
        if not HAS_EMBEDDINGS:
            return
        
        with get_session() as session:
            if knowledge_base_id is not None:
                chunks = session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
                total_chunks = session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).count()
            else:
                chunks = session.query(KnowledgeChunk).all()
                total_chunks = session.query(KnowledgeChunk).count()
            
            if not chunks:
                return
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏ –ø–æ knowledge_base_id –∏ –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å coverage
        chunks_by_kb = defaultdict(list)
        chunks_with_embedding = 0
        expected_dim = None
        dim_mismatches = 0
        
        for chunk in chunks:
            if chunk.embedding:
                try:
                    embedding = np.array(json.loads(chunk.embedding))
                    embedding_dim = embedding.shape[0] if len(embedding.shape) == 1 else embedding.shape[1]
                    
                    # –ó–∞–ø–æ–º–Ω–∏—Ç—å expected_dim –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –≤–∞–ª–∏–¥–Ω–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–µ
                    if expected_dim is None:
                        expected_dim = embedding_dim
                    
                    # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –¥—Ä—É–≥–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é
                    if embedding_dim != expected_dim:
                        dim_mismatches += 1
                        logger.warning(
                            f"Skipping chunk {chunk.id}: embedding dimension {embedding_dim} != expected {expected_dim}"
                        )
                        continue
                    
                    chunks_by_kb[chunk.knowledge_base_id].append((chunk, embedding))
                    chunks_with_embedding += 1
                except Exception as e:
                    logger.debug(f"Failed to parse embedding for chunk {chunk.id}: {e}")
                    continue
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if dim_mismatches > 0:
            logger.warning(
                f"Skipped {dim_mismatches} chunks with dimension mismatch (expected {expected_dim})"
            )
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å coverage –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        if total_chunks > 0:
            coverage_pct = (chunks_with_embedding / total_chunks) * 100
            kb_info = f"KB {knowledge_base_id}" if knowledge_base_id is not None else "all KBs"
            logger.info(
                f"Index coverage for {kb_info}: {chunks_with_embedding}/{total_chunks} chunks with embeddings ({coverage_pct:.1f}%)"
            )
            if coverage_pct < 50:
                logger.warning(f"Low embedding coverage ({coverage_pct:.1f}%) - many chunks will fall back to keyword search")
        
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–π KB –æ—Ç–¥–µ–ª—å–Ω–æ
        for kb_id, chunk_emb_pairs in chunks_by_kb.items():
            if not chunk_emb_pairs:
                continue
            
            valid_chunks = [pair[0] for pair in chunk_emb_pairs]
            embeddings = np.array([pair[1] for pair in chunk_emb_pairs]).astype('float32')
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è cosine similarity
            faiss.normalize_L2(embeddings)
            
            dimension = embeddings.shape[1]
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å IndexFlatIP (Inner Product) –¥–ª—è cosine similarity
            index = faiss.IndexFlatIP(dimension)
            index.add(embeddings)
            
            self.index_by_kb[kb_id] = index
            self.chunks_by_kb[kb_id] = valid_chunks
        
        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω –æ–±—â–∏–π –∏–Ω–¥–µ–∫—Å
        if knowledge_base_id is None and chunks_by_kb:
            # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ —á–∞–Ω–∫–∏ –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ API
            all_chunks = []
            all_embeddings = []
            for kb_id, chunk_emb_pairs in chunks_by_kb.items():
                for chunk, emb in chunk_emb_pairs:
                    all_chunks.append(chunk)
                    all_embeddings.append(emb)
            
            if all_embeddings:
                self.chunks = all_chunks
                all_embeddings = np.array(all_embeddings).astype('float32')
                faiss.normalize_L2(all_embeddings)
                self.dimension = all_embeddings.shape[1]
                self.index = faiss.IndexFlatIP(self.dimension)
                self.index.add(all_embeddings)
    
    def add_knowledge_base(self, name: str, description: str = "") -> KnowledgeBase:
        """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        with _db_write_lock:
            with get_session() as session:
                kb = KnowledgeBase(name=name, description=description)
                session.add(kb)
                session.flush()  # –ü–æ–ª—É—á–∏—Ç—å ID
                session.refresh(kb)
                return kb
    
    def get_knowledge_base(self, name_or_id) -> Optional[KnowledgeBase]:
        """–ü–æ–ª—É—á–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ –∏–º–µ–Ω–∏ –∏–ª–∏ ID"""
        with get_session() as session:
            if isinstance(name_or_id, int):
                return session.query(KnowledgeBase).filter_by(id=name_or_id).first()
            return session.query(KnowledgeBase).filter_by(name=name_or_id).first()
    
    def list_knowledge_bases(self) -> List[KnowledgeBase]:
        """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π"""
        with get_session() as session:
            return session.query(KnowledgeBase).all()
    
    def add_chunk(self, knowledge_base_id: int, content: str, 
                  source_type: str = "text", source_path: str = "",
                  metadata: Optional[Dict] = None) -> KnowledgeChunk:
        """–î–æ–±–∞–≤–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∑–Ω–∞–Ω–∏—è —Å retry –ª–æ–≥–∏–∫–æ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ –ë–î"""
        import time
        import random
        max_retries = 10  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        base_delay = 0.2  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 0.05 –¥–æ 0.2 —Å–µ–∫—É–Ω–¥—ã
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å embedding –∑–∞—Ä–∞–Ω–µ–µ, —á—Ç–æ–±—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–º—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        embedding = self._get_embedding(content)
        embedding_json = json.dumps(embedding.tolist()) if embedding is not None else None
        
        with _db_write_lock:
            for attempt in range(max_retries):
                try:
                    with get_session() as session:
                        chunk = KnowledgeChunk(
                            knowledge_base_id=knowledge_base_id,
                            content=content,
                            chunk_metadata=json.dumps(metadata or {}),
                            embedding=embedding_json,
                            source_type=source_type,
                            source_path=source_path
                        )
                        session.add(chunk)
                        session.flush()  # –ü–æ–ª—É—á–∏—Ç—å ID
                        session.refresh(chunk)
                    
                    # –£–±—Ä–∞—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ - –∏–Ω–¥–µ–∫—Å –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É
                    # –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Å cosine similarity –∏ per-KB –∏–Ω–¥–µ–∫—Å–∞–º–∏
                    if embedding is not None and HAS_EMBEDDINGS:
                        self.index = None
                        self.chunks = []
                        self.index_by_kb.clear()
                        self.chunks_by_kb.clear()
                    
                    return chunk
                except Exception as e:
                    if "locked" in str(e).lower() or "database is locked" in str(e):
                        if attempt < max_retries - 1:
                            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff —Å –¥–∂–∏—Ç—Ç–µ—Ä–æ–º
                            delay = base_delay * (2 ** attempt)
                            jitter = delay * 0.2 * (random.random() * 2 - 1)
                            delay_with_jitter = max(0.1, delay + jitter)
                            logger.warning(
                                f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}, "
                                f"–ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay_with_jitter:.2f}—Å (timeout=60s, busy_timeout=60000ms)"
                            )
                            time.sleep(delay_with_jitter)
                            continue
                        else:
                            logger.error(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å —á–∞–Ω–∫ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e} "
                                f"(timeout=60s, busy_timeout=60000ms)"
                            )
                            raise
                    else:
                        raise
    
    def add_chunks_batch(self, chunks_data: List[Dict]) -> List[KnowledgeChunk]:
        """
        –î–æ–±–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∑–Ω–∞–Ω–∏—è –ø–∞–∫–µ—Ç–Ω–æ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è SQLite)
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—Ñ–∞–∑–Ω—É—é –∑–∞–ø–∏—Å—å:
        1. –í—Å—Ç–∞–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –±–µ–∑ embedding (–±—ã—Å—Ç—Ä–æ)
        2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ embedding –±–∞—Ç—á–∞–º–∏ (–º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
        
        Args:
            chunks_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–æ–≤:
                {
                    'knowledge_base_id': int,
                    'content': str,
                    'source_type': str,
                    'source_path': str,
                    'metadata': dict (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                }
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö KnowledgeChunk –æ–±—ä–µ–∫—Ç–æ–≤
        """
        import time
        import random
        max_retries = 10  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        base_delay = 0.2  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 0.02 –¥–æ 0.2 —Å–µ–∫—É–Ω–¥—ã
        batch_size = 50  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è bulk –æ–ø–µ—Ä–∞—Ü–∏–π
        
        with _db_write_lock:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Å–µ embeddings –∑–∞—Ä–∞–Ω–µ–µ (–¥–æ –ª—é–±—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π)
            prepared_data = []
            for chunk_data in chunks_data:
                content = chunk_data.get('content', '')
                embedding = self._get_embedding(content)
                embedding_json = json.dumps(embedding.tolist()) if embedding is not None else None
                prepared_data.append((chunk_data, embedding, embedding_json))
            
            all_chunks = []
            chunks_with_embeddings = []  # (chunk_id, embedding_json, embedding) –¥–ª—è –≤—Ç–æ—Ä–æ–π —Ñ–∞–∑—ã
            
            # –§–∞–∑–∞ 1: –í—Å—Ç–∞–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –±–µ–∑ embedding (–±—ã—Å—Ç—Ä–æ, –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
            for batch_start in range(0, len(prepared_data), batch_size):
                batch_data = prepared_data[batch_start:batch_start + batch_size]
                
                for attempt in range(max_retries):
                    try:
                        with get_session() as session:
                            chunks_to_add = []
                            batch_embeddings = []
                            
                            for chunk_data, embedding, embedding_json in batch_data:
                                chunk = KnowledgeChunk(
                                    knowledge_base_id=chunk_data['knowledge_base_id'],
                                    content=chunk_data.get('content', ''),
                                    chunk_metadata=json.dumps(chunk_data.get('metadata') or {}),
                                    embedding=None,  # –í—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ embedding —Å–Ω–∞—á–∞–ª–∞
                                    source_type=chunk_data.get('source_type', 'text'),
                                    source_path=chunk_data.get('source_path', '')
                                )
                                chunks_to_add.append(chunk)
                                if embedding_json:
                                    batch_embeddings.append((chunk, embedding_json, embedding))
                            
                            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å add_all –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                            session.add_all(chunks_to_add)
                            session.flush()  # –ü–æ–ª—É—á–∏—Ç—å IDs
                            
                            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —á–∞–Ω–∫–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ C)
                            all_chunks.extend(chunks_to_add)
                            
                            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–ª—è –≤—Ç–æ—Ä–æ–π —Ñ–∞–∑—ã (ID –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ flush)
                            for chunk, emb_json, emb in batch_embeddings:
                                # ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ—Å–ª–µ flush
                                if hasattr(chunk, 'id') and chunk.id:
                                    chunks_with_embeddings.append((chunk.id, emb_json, emb))
                                else:
                                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å ID –¥–ª—è —á–∞–Ω–∫–∞, –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω embedding")
                        
                        break  # –£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ
                    except Exception as e:
                        if "locked" in str(e).lower() or "database is locked" in str(e):
                            if attempt < max_retries - 1:
                                delay = base_delay * (2 ** attempt)
                                if attempt == 0:
                                    logger.warning(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –±–∞—Ç—á–∞ {batch_start//batch_size + 1}, –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {delay:.2f}—Å")
                                time.sleep(delay)
                                continue
                            else:
                                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –±–∞—Ç—á –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                                raise
                        else:
                            raise
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                if batch_start + batch_size < len(prepared_data):
                    time.sleep(0.01)
            
            # –§–∞–∑–∞ 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ embedding –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø–∞—á–∫–∞–º–∏ —Å commit –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ B)
            if chunks_with_embeddings:
                embedding_batch_size = 30  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
                for batch_start in range(0, len(chunks_with_embeddings), embedding_batch_size):
                    batch_embeddings = chunks_with_embeddings[batch_start:batch_start + embedding_batch_size]
                    
                    for attempt in range(max_retries):
                        try:
                            # –ö–æ—Ä–æ—Ç–∫–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è: update + commit
                            with get_session() as session:
                                for chunk_id, embedding_json, _ in batch_embeddings:
                                    session.query(KnowledgeChunk).filter_by(id=chunk_id).update(
                                        {'embedding': embedding_json}
                                    )
                                # commit –≤—ã–ø–æ–ª–Ω–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∏–∑ with
                            break
                        except Exception as e:
                            if "locked" in str(e).lower() or "database is locked" in str(e):
                                if attempt < max_retries - 1:
                                    # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff —Å –¥–∂–∏—Ç—Ç–µ—Ä–æ–º
                                    delay = base_delay * (2 ** attempt)
                                    jitter = delay * 0.2 * (random.random() * 2 - 1)
                                    delay_with_jitter = max(0.1, delay + jitter)
                                    logger.warning(
                                        f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ embedding –±–∞—Ç—á–∞, "
                                        f"–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay_with_jitter:.2f}—Å"
                                    )
                                    time.sleep(delay_with_jitter)
                                    continue
                                else:
                                    logger.error(
                                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å embedding –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e} "
                                        f"(timeout=60s, busy_timeout=60000ms)"
                                    )
                                    # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
                                    break
                            else:
                                raise
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ D: –æ—Ç–∫–ª—é—á–∏—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ—Å–ª–µ –±–∞—Ç—á–∞
            # –ò–Ω–¥–µ–∫—Å –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ _load_index()
            # –≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç —á–∏—Å–ª–æ –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –ë–î –≤ –º–æ–º–µ–Ω—Ç –∑–∞–ø–∏—Å–∏
            # –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ –ø—Ä–æ—Å—Ç–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
            if HAS_EMBEDDINGS and chunks_with_embeddings:
                self.index = None
                self.chunks = []
                # –û—á–∏—Å—Ç–∏—Ç—å –∏–Ω–¥–µ–∫—Å—ã –ø–æ KB
                self.index_by_kb.clear()
                self.chunks_by_kb.clear()
                # –ò–Ω–¥–µ–∫—Å –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–±—Ä–∞–Ω –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –ø–æ–∏—Å–∫–µ —á–µ—Ä–µ–∑ _load_index()
        
        return all_chunks
    
    def search(self, query: str, knowledge_base_id: Optional[int] = None, 
               top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π (dense + keyword —Å –≤–æ–∑–º–æ–∂–Ω—ã–º rerank)."""
        import re
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ how-to –±—É—Å—Ç–∞—Ö
        query_words = re.findall(r'\w+', query.lower())
        
        # –ï—Å–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        if not HAS_EMBEDDINGS or not self.encoder:
            return self._simple_search(query, knowledge_base_id, top_k)
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏)
        if knowledge_base_id is not None:
            if knowledge_base_id not in self.index_by_kb:
                self._load_index(knowledge_base_id)
        else:
            if not self.index_by_kb:
                self._load_index(None)
        
        # –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        query_embedding = self._get_embedding(query)
        if query_embedding is None:
            return self._simple_search(query, knowledge_base_id, top_k)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏–Ω–¥–µ–∫—Å –ø–æ KB –∏–ª–∏ –æ–±—â–∏–π
        if knowledge_base_id is not None:
            if knowledge_base_id not in self.index_by_kb:
                return self._simple_search(query, knowledge_base_id, top_k)
            index = self.index_by_kb[knowledge_base_id]
            chunks = self.chunks_by_kb[knowledge_base_id]
        else:
            if self.index is None or len(self.chunks) == 0:
                return self._simple_search(query, knowledge_base_id, top_k)
            index = self.index
            chunks = self.chunks
        
        if len(chunks) == 0:
            return []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å query embedding –¥–ª—è cosine similarity
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ (how-to –∏–ª–∏ –æ–±—ã—á–Ω—ã–π)
        is_howto_query = self._is_howto_query(query)
        
        # Dense‚Äë–ø–æ–∏—Å–∫: —à–∏—Ä–æ–∫–∏–π –ø—É–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        # –î–ª—è how-to –∑–∞–ø—Ä–æ—Å–æ–≤ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º candidate_k (–¥–æ 300-500 –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞–∑)
        if is_howto_query:
            candidate_k = min(max(300, self.max_candidates * 3), len(chunks), 500)
        else:
            candidate_k = min(self.max_candidates, len(chunks))
        scores, indices = index.search(query_embedding, candidate_k)
        
        dense_candidates: List[Dict] = []
        for i, idx in enumerate(indices[0]):
            if idx < len(chunks):
                chunk = chunks[idx]
                # KB —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞
                metadata = json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {}
                
                # –î–ª—è how-to –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–∞–µ–º –±—É—Å—Ç code/list —á–∞–Ω–∫–∞–º
                similarity = float(scores[0][i])  # –≠—Ç–æ —É–∂–µ cosine similarity (inner product, –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç -1 –¥–æ 1)
                if is_howto_query:
                    chunk_kind = metadata.get("chunk_kind", "text")
                    if chunk_kind in ("code", "list"):
                        similarity *= 1.5  # –ë—É—Å—Ç –¥–ª—è code/list –≤ how-to —Ä–µ–∂–∏–º–µ
                    
                    # –ë—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ section_path
                    section_path = (metadata.get("section_path") or "").lower()
                    if section_path and any(word in section_path for word in query_words):
                        similarity *= 1.2
                
                # –î–ª—è cosine similarity: distance = -similarity (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é distance = –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity)
                distance = -similarity
                
                dense_candidates.append(
                    {
                        "content": chunk.content,
                        "metadata": metadata,
                        "source_type": chunk.source_type,
                        "source_path": chunk.source_path,
                        "distance": distance,
                        "similarity": similarity,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º similarity –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        "origin": "dense",
                    }
                )

        # Keyword‚Äë–ø–æ–∏—Å–∫ (BM25‚Äë–ø–æ–¥–æ–±–Ω—ã–π) –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        keyword_candidates = self._simple_search(
            query,
            knowledge_base_id=knowledge_base_id,
            top_k=self.max_candidates,
        )
        for kc in keyword_candidates:
            kc.setdefault("origin", "bm25")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –ø–æ (source_path, content)
        merged: List[Dict] = []
        seen = set()
        for cand in dense_candidates + keyword_candidates:
            key = (cand.get("source_path") or "", (cand.get("content") or "")[:200])
            if key in seen:
                continue
            seen.add(key)
            merged.append(cand)

        if not merged:
            return []

        # –ï—Å–ª–∏ –µ—Å—Ç—å reranker ‚Äì –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –±–µ—Ä—ë–º top_k –ø–æ score
        if HAS_RERANKER and self.reranker is not None:
            try:
                pairs = [[query, c.get("content", "")] for c in merged]
                scores = self.reranker.predict(pairs)

                scored = list(zip(merged, scores))
                scored.sort(key=lambda x: x[1], reverse=True)
                top = scored[: top_k]

                logger.debug(
                    "Reranker –ø—Ä–∏–º–µ–Ω–µ–Ω: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ %d –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –≤—ã–±—Ä–∞–Ω–æ top-%d",
                    len(merged),
                    len(top),
                )
                if top:
                    logger.debug(
                        "–õ—É—á—à–∏–π rerank_score: %.4f, —Ö—É–¥—à–∏–π: %.4f",
                        float(top[0][1]),
                        float(top[-1][1]),
                    )

                results = []
                for cand, score in top:
                    results.append(
                        {
                            "content": cand.get("content", ""),
                            "metadata": cand.get("metadata") or {},
                            "source_type": cand.get("source_type"),
                            "source_path": cand.get("source_path"),
                            "distance": float(cand.get("distance", 0.0)),
                            "rerank_score": float(score),
                            "origin": cand.get("origin"),
                        }
                    )
                return results
            except Exception as e:
                logger.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã reranker, –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–µ–≥–æ: %s", e)
                import traceback
                logger.debug("Traceback reranker: %s", traceback.format_exc())
                # fallthrough –∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ merged (dense + keyword)
        
        # –ï—Å–ª–∏ reranker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî fallback: —Å–º–µ—à–∞–Ω–Ω—ã–π —Ä–∞–Ω–∂–∏—Ä –¥–ª—è how-to –∑–∞–ø—Ä–æ—Å–æ–≤
        # –î–ª—è how-to: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç code/list, –∑–∞—Ç–µ–º bm25, –∑–∞—Ç–µ–º distance
        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ distance
        if is_howto_query:
            # –î–ª—è how-to: is_code_or_list (code/list –≤—ã—à–µ) ‚Üí origin_priority (bm25 –≤—ã—à–µ) ‚Üí distance
            def sort_key_howto(c):
                metadata = c.get("metadata") or {}
                chunk_kind = metadata.get("chunk_kind", "text")
                is_code_or_list = 0 if chunk_kind in ("code", "list") else 1  # code/list = 0 (–≤—ã—à–µ)
                origin_priority = 0 if c.get("origin") == "bm25" else 1  # bm25 = 0 (–≤—ã—à–µ), dense = 1
                distance = float(c.get("distance", float("inf")))
                return (is_code_or_list, origin_priority, distance)
            merged_sorted = sorted(merged, key=sort_key_howto)[: top_k]
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: —Ç–æ–ª—å–∫–æ –ø–æ distance
            merged_sorted = sorted(merged, key=lambda c: float(c.get("distance", float("inf"))))[: top_k]
        
        results = []
        for cand in merged_sorted:
            results.append(
                {
                    "content": cand.get("content", ""),
                    "metadata": cand.get("metadata") or {},
                    "source_type": cand.get("source_type"),
                    "source_path": cand.get("source_path"),
                    "distance": float(cand.get("distance", 0.0)),
                    "origin": cand.get("origin", "dense"),
                }
            )
        return results
    
    def _is_howto_query(self, query: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–ø—Ä–æ—Å–æ–º —Ç–∏–ø–∞ 'how-to' (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è/–ø—Ä–æ—Ü–µ–¥—É—Ä–∞)."""
        import re
        query_lower = query.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ how-to –∑–∞–ø—Ä–æ—Å
        howto_keywords = [
            'how to', 'howto', 'how do', 'how can', 'how should',
            'initialize', 'init', 'setup', 'set up', 'install', 'configure',
            'create', 'build', 'compile', 'sync', 'sync and build',
            'run', 'execute', 'start', 'begin', 'get started',
            'tutorial', 'guide', 'steps', 'procedure', 'process',
            'command', 'example', 'demo'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ how-to –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        for keyword in howto_keywords:
            if keyword in query_lower:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å", "–∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å" –∏ —Ç.–¥.
        russian_howto_patterns = [
            r'–∫–∞–∫\s+(—Å–¥–µ–ª–∞—Ç—å|—Å–æ–∑–¥–∞—Ç—å|–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å|—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å|–∑–∞–ø—É—Å—Ç–∏—Ç—å|–Ω–∞—á–∞—Ç—å)',
            r'–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è',
            r'—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ',
            r'—à–∞–≥–∏',
        ]
        for pattern in russian_howto_patterns:
            if re.search(pattern, query_lower):
                return True
        
        return False
    
    def _simple_search(self, query: str, knowledge_base_id: Optional[int] = None, 
                      top_k: int = 5) -> List[Dict]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        import re
        import json
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º
        query_lower = query.lower()
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º, –∞–º–ø–µ—Ä—Å–∞–Ω–¥–∞–º, –¥–µ—Ñ–∏—Å–∞–º –∏ –¥—Ä—É–≥–∏–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
        query_words = re.findall(r'\w+', query_lower)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
        is_howto = self._is_howto_query(query)
        
        # –°–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è how-to –∑–∞–ø—Ä–æ—Å–æ–≤ (–∫–æ–º–∞–Ω–¥—ã, —Ñ–ª–∞–≥–∏, –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
        strong_tokens = ['repo', '--depth', '--reference', 'mkdir', 'cd', 'git', 'init', 'sync', 
                        'build', 'compile', 'install', 'docker', 'npm', 'yarn', 'pip', 'apt', 'yum']
        
        with get_session() as session:
            # –î–ª—è how-to –∑–∞–ø—Ä–æ—Å–æ–≤ —Å —Å–∏–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π SQL-—Ñ–∏–ª—å—Ç—Ä
            if is_howto:
                # –ù–∞–π—Ç–∏ —Å–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–µ
                found_strong_tokens = [token for token in strong_tokens if token in query_lower]
                
                if found_strong_tokens:
                    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å SQL-—Ñ–∏–ª—å—Ç—Ä: content LIKE '%token%' OR content LIKE '%token%' ...
                    filters = []
                    for token in found_strong_tokens:
                        # –ò—â–µ–º —Ç–æ–∫–µ–Ω –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –∫–∞–∫ —á–∞—Å—Ç—å –∫–æ–º–∞–Ω–¥—ã
                        filters.append(KnowledgeChunk.content.like(f'%{token}%'))
                    
                    # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ KB
                    query_obj = session.query(KnowledgeChunk)
                    if knowledge_base_id is not None:
                        query_obj = query_obj.filter_by(knowledge_base_id=knowledge_base_id)
                    
                    # –ü—Ä–∏–º–µ–Ω–∏—Ç—å SQL-—Ñ–∏–ª—å—Ç—Ä –ø–æ —Å–∏–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–º
                    chunks = query_obj.filter(or_(*filters)).all()
                    logger.debug(f"Pre-filtered {len(chunks)} chunks using strong tokens: {found_strong_tokens}")
                else:
                    # –ù–µ—Ç —Å–∏–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ - –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
                    if knowledge_base_id is not None:
                        chunks = session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
                    else:
                        chunks = session.query(KnowledgeChunk).all()
            else:
                # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ - –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
                if knowledge_base_id is not None:
                    chunks = session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
                else:
                    chunks = session.query(KnowledgeChunk).all()
        
        scored_chunks = []
        for chunk in chunks:
            content_lower = chunk.content.lower()
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º source_path –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–º–µ–Ω–∞–º —Ñ–∞–π–ª–æ–≤
            source_path_lower = (chunk.source_path or "").lower()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            metadata = {}
            try:
                if chunk.chunk_metadata:
                    metadata = json.loads(chunk.chunk_metadata)
            except:
                pass
            
            # –ü–æ–∏—Å–∫ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ section_title (–≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
            title_lower = (metadata.get("title") or "").lower()
            section_title_lower = (metadata.get("section_title") or "").lower()
            section_path_lower = (metadata.get("section_path") or "").lower()
            chunk_kind = metadata.get("chunk_kind", "text")
            
            # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            content_score = sum(1 for word in query_words if word in content_lower)
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ (–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ)
            title_score = sum(2 for word in query_words if word in title_lower)
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ section_title
            section_score = sum(1.5 for word in query_words if word in section_title_lower)
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ section_path
            section_path_score = sum(1.5 for word in query_words if word in section_path_lower)
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞/–ø—É—Ç–∏
            path_score = sum(1 for word in query_words if word in source_path_lower)
            
            # –î–ª—è how-to –∑–∞–ø—Ä–æ—Å–æ–≤: –±—É—Å—Ç –¥–ª—è code/list —á–∞–Ω–∫–æ–≤
            chunk_kind_boost = 0
            if is_howto and chunk_kind in ("code", "list"):
                chunk_kind_boost = 3
            
            # –ü–æ–∏—Å–∫ –∫–æ–º–∞–Ω–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ (–¥–ª—è how-to)
            command_score = 0
            if is_howto:
                command_pattern = r'(^|\n)(repo|git|mkdir|cd|python|docker|npm|yarn|pip|apt|yum)\b'
                if re.search(command_pattern, chunk.content, re.IGNORECASE):
                    command_score = 2
            
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã (–¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ "Initialize repository and sync code")
            phrase_in_content = query_lower in content_lower
            phrase_in_title = query_lower in title_lower
            phrase_in_section = query_lower in section_title_lower
            phrase_in_section_path = query_lower in section_path_lower
            
            total_score = (
                content_score + 
                title_score * 3 +  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—á–µ–Ω—å –≤–∞–∂–µ–Ω
                section_score * 2 +
                section_path_score * 2.5 +  # section_path –≤–∞–∂–µ–Ω –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
                path_score * 2 +
                chunk_kind_boost +
                command_score +
                (10 if phrase_in_title else 0) +  # –ë–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
                (8 if phrase_in_section_path else 0) +
                (5 if phrase_in_section else 0) +
                (3 if phrase_in_content else 0)
            )
            
            if total_score > 0:
                scored_chunks.append((total_score, chunk))
        
        scored_chunks.sort(reverse=True, key=lambda x: x[0])
        
        results = []
        for score, chunk in scored_chunks[:top_k]:
            results.append({
                'content': chunk.content,
                'metadata': json.loads(chunk.chunk_metadata) if chunk.chunk_metadata else {},
                'source_type': chunk.source_type,
                'source_path': chunk.source_path,
                'distance': 1.0 / (score + 1)  # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            })
        
        return results
    
    def delete_knowledge_base(self, knowledge_base_id: int) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π, –≤—Å–µ –µ—ë —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ –∂—É—Ä–Ω–∞–ª –∑–∞–≥—Ä—É–∑–æ–∫"""
        with _db_write_lock:
            with get_session() as session:
                # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                chunks = session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
                for chunk in chunks:
                    session.delete(chunk)
                
                # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∂—É—Ä–Ω–∞–ª–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –¥–ª—è —ç—Ç–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
                logs = session.query(KnowledgeImportLog).filter_by(knowledge_base_id=knowledge_base_id).all()
                for log in logs:
                    session.delete(log)
                
                # –£–¥–∞–ª–∏—Ç—å —Å–∞–º—É –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
                kb = session.query(KnowledgeBase).filter_by(id=knowledge_base_id).first()
                if kb:
                    session.delete(kb)
                    session.flush()
                
                if not kb:
                    return False
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å (–≤–Ω–µ —Å–µ—Å—Å–∏–∏)
            self.chunks = []
            self.index = None
            self.index_by_kb.clear()
            self.chunks_by_kb.clear()
            self._load_index()
            return True
    
    def clear_knowledge_base(self, knowledge_base_id: int) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –æ—Ç –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ –∂—É—Ä–Ω–∞–ª–∞ –∑–∞–≥—Ä—É–∑–æ–∫"""
        with _db_write_lock:
            with get_session() as session:
                # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                chunks = session.query(KnowledgeChunk).filter_by(knowledge_base_id=knowledge_base_id).all()
                for chunk in chunks:
                    session.delete(chunk)
                
                # –£–¥–∞–ª–∏—Ç—å –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∂—É—Ä–Ω–∞–ª–∞ –∑–∞–≥—Ä—É–∑–æ–∫ –¥–ª—è —ç—Ç–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
                logs = session.query(KnowledgeImportLog).filter_by(knowledge_base_id=knowledge_base_id).all()
                for log in logs:
                    session.delete(log)
                session.flush()
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å (–≤–Ω–µ —Å–µ—Å—Å–∏–∏)
            self.chunks = []
            self.index = None
            self.index_by_kb.clear()
            self.chunks_by_kb.clear()
            self._load_index()
            return True

    def delete_chunks_by_source_exact(
        self,
        knowledge_base_id: int,
        source_type: str,
        source_path: str,
    ) -> int:
        """
        –£–¥–∞–ª–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ —Ä–∞–º–∫–∞—Ö –ë–ó.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –∑–∞–º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ.
        """
        if not source_path:
            return 0

        with _db_write_lock:
            with get_session() as session:
                q = (
                    session.query(KnowledgeChunk)
                    .filter_by(
                        knowledge_base_id=knowledge_base_id,
                        source_type=source_type,
                        source_path=source_path,
                    )
                )
                chunks = q.all()
                deleted = 0
                for chunk in chunks:
                    session.delete(chunk)
                    deleted += 1
                session.flush()

            if deleted:
                # –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª —Ç–µ–∫—É—â–µ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –ë–î
                self.chunks = []
                self.index = None
                # –£–¥–∞–ª–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —ç—Ç–æ–π KB
                if knowledge_base_id in self.index_by_kb:
                    del self.index_by_kb[knowledge_base_id]
                if knowledge_base_id in self.chunks_by_kb:
                    del self.chunks_by_kb[knowledge_base_id]
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —ç—Ç–æ–π KB
                self._load_index(knowledge_base_id)

            return deleted

    def delete_chunks_by_source_prefix(
        self,
        knowledge_base_id: int,
        source_type: str,
        source_prefix: str,
    ) -> int:
        """
        –£–¥–∞–ª–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∑–Ω–∞–Ω–∏–π –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–¥–Ω–æ–π –≤–∏–∫–∏).
        
        –≠—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è wiki-—Å–∫—Ä–µ–ø–µ—Ä–æ–º –¥–ª—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏ –≤–∏–∫–∏ –±–µ–∑ –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
        """
        if not source_prefix:
            return 0

        with _db_write_lock:
            with get_session() as session:
                # –ù–∞–π—Ç–∏ –≤—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏ —Å –Ω—É–∂–Ω—ã–º —Ç–∏–ø–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                query = (
                    session.query(KnowledgeChunk)
                    .filter_by(knowledge_base_id=knowledge_base_id, source_type=source_type)
                )
                chunks = query.all()
                deleted = 0

                for chunk in chunks:
                    if chunk.source_path and chunk.source_path.startswith(source_prefix):
                        session.delete(chunk)
                        deleted += 1
                session.flush()

            if deleted:
                # –ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã –æ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª —Ç–µ–∫—É—â–µ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –ë–î
                self.chunks = []
                self.index = None
                # –£–¥–∞–ª–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —ç—Ç–æ–π KB
                if knowledge_base_id in self.index_by_kb:
                    del self.index_by_kb[knowledge_base_id]
                if knowledge_base_id in self.chunks_by_kb:
                    del self.chunks_by_kb[knowledge_base_id]
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —ç—Ç–æ–π KB
                self._load_index(knowledge_base_id)

            return deleted


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã
rag_system = RAGSystem()

